#!/bin/python3
import functools
import glob
import itertools
import json
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
from matplotlib import ticker
from matplotlib.lines import Line2D

from tests.topotests.bmp_benchmark_test.frr_memuse_log_parse import _get_column_titles, \
    _get_col_index, _get_column, _get_column_types, _get_column_display_name

###

# need to ensure change when doing prefix announcements because
# ingress[UPDT/WITHDRW] => outgress[UPDT/WITHDRW] or None if no change
#

# when leaking between vrf
# which messages are from leaking ? peerid in list[router_id]
# outgress from vrf src => ingress from vrf dest => outgress to peers
# event match in vrf dest is ingress from vrf dest to outgress to peers
# need to match ingress.is_leak && !outgress.is_leak && ingress.pfx == outgress.pfx && ingress.time < outgress.time
###


CWD = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(CWD, "../"))


def load_files_from_router(rname):
    fdir = os.path.join(CWD, rname, "benchmark")
    print("Loading files from router {} in directory {}".format(rname, fdir))
    benchmark_logs = list()
    for file in glob.glob(os.path.join(fdir, "benchmark_*_vrf_*")):
        with open(file) as file:
            res = json.load(file)
            # eliminate last empty event
            res["events"] = [x for x in res.get("events") if bool(x)]
            leaks = [x for x in res.get("events") if bool(x.get("leak"))]
            # res["events"] = [x for x in res.get("events") if not bool(x.get("leak"))]
            # print(leaks)

            benchmark_logs.append(res)

    ram_usage = []
    for file in glob.glob(os.path.join(fdir, "ram_usage")):
        with open(file) as file:
            ram_usage = json.load(file)

    return benchmark_logs, ram_usage


# take object {timestamp:number, events:[]}
# "normalize" timestamp (set time reference to lowest event timestamp)
# return 2 list of all ingress and outgress events separated
def unzip_ingress_outgress(vrf_log_json):
    def _normalize_timestamp(event, ref_timestamp):
        event["timestamp"] = event["timestamp"] - ref_timestamp
        return event

    ref_timestamp = vrf_log_json.get("events")[0].get("timestamp")
    return [_normalize_timestamp(x, ref_timestamp) for x in vrf_log_json.get("events") if x.get("ingress") == 1], \
           [_normalize_timestamp(x, ref_timestamp) for x in vrf_log_json.get("events") if
            x.get("ingress") == 0 and x.get("leak") == 0], \
           [x for x in vrf_log_json.get("events")]


# take object {timestamp:number, events:[])}
# separate ingress and outgress
# find next outgress msg related to prefix of each ingress msg
# return [ingress, next_outgress]
def match_events(vrf_info, vrf_log_json):
    def _find_prefix_after_msg(events_list, trig_event):
        return next(filter(
            lambda event: event.get("timestamp") >= trig_event.get("timestamp")
                          and event.get("prefix") == trig_event.get("prefix")
                          #     after a non leak update/withdraw we expect
                          #     either a leak to another vrf
                          #     or an outgoing update/withdraw for the same afi:safi
                          and (_is_leak(event) or
                               (not _is_leak(event) and event.get("afi_safi") == trig_event.get("afi_safi"))
                               ),
            events_list), None)

    def _is_leak(event):
        return bool(event.get("leak"))

    def _find_prefix_after_leak(events_all, trig_event):
        # ingress.is_leak && !outgress.is_leak && ingress.pfx == outgress.pfx && ingress.time < outgress.time
        return next(filter(
            lambda event: _is_leak(trig_event)
                          and not _is_leak(event)
                          and event.get("timestamp") >= trig_event.get("timestamp")
                          and event.get("prefix") == trig_event.get("prefix")
                          and event.get("afi_safi") == trig_event.get("afi_safi"),
            # TODO add rd equals check (shouldn't change anything)
            events_all), None)

    ingress, outgress, normalized_all = unzip_ingress_outgress(vrf_log_json)

    return [(ingress_x, (_find_prefix_after_msg(outgress, ingress_x)
                         if not _is_leak(ingress_x)
                         else _find_prefix_after_leak(normalized_all, ingress_x)
                         )) for ingress_x in ingress]


# take [{"total": int, "details": []}]
# get every mem_usage field in each event
# return list of mem_usage values (memory usage of logging system)
def benchmark_logs_get_ram_usage(lmlogs_module_ram_usage):
    return list(map(_get_total_from_dict, lmlogs_module_ram_usage["lmlogs"]))


# take list [{},{},{}] with n fields in each list member
# return dict with n lists containing all values of the same key, in order mapped to list member keys
def json_obj_split_series(obj):
    return dict(zip(obj[0].keys(), zip(*[r.values() for r in obj])))


def make_bins_from_timeseries(values, nbin=None, binsize=None):
    if nbin is None and binsize is None:
        raise ValueError()

    values = np.array(values)
    bin_size = values.size / nbin
    indexes = np.arange(0, nbin * int(np.ceil(bin_size))).reshape((nbin, int(np.ceil(bin_size))))
    binned = np.where(indexes < values.size, np.take(values, indexes, mode="clip"), -1)
    undefined = np.any(binned < 0, axis=1)
    binned[undefined] = np.where(binned[undefined],
                                 np.nan_to_num(np.mean(binned[undefined], where=binned[undefined] >= 0), nan=0),
                                 binned[undefined])

    return np.array(list(range(nbin))), np.mean(binned, axis=1)


def format_duration_for_display(duration):
    return str(np.round(duration, 2))


def timestamps_to_duration(timeseries_timestamps):
    duration_s = np.max(timeseries_timestamps - np.min(timeseries_timestamps)) * 1e-9
    return duration_s, format_duration_for_display(duration_s)


def scale_x_axis_for_duration(x_axis, duration):
    return x_axis * duration / float(len(x_axis))


def make_x_axis_for_timeseries(y_values, duration):
    return scale_x_axis_for_duration(np.arange(len(y_values)), duration)


def make_plot_title(title, sample_count, duration_txt, tyqe):
    return f"{title}\n{sample_count} samples over {duration_txt} seconds\n{tyqe}"


def plot_data_stats(axis, data, data_label=None, mean=False, median=False):
    if mean: axis.axhline(y=np.nanmean(data), color=plt_colors[len(axis.lines)],
                          label=f"mean of {data_label}" if data_label is not None else None)
    if median: axis.axhline(y=np.nanmedian(data), color=plt_colors[len(axis.lines)],
                            label=f"median of {data_label}" if data_label is not None else None)


def sync_timeseries(reference, series_list):
    return np.array([np.interp(reference, series[:, 0], series[:, 1]) for series in series_list])


_get_total_from_dict = functools.partial(lambda key, dic: dic[key], "total")


def plot_ram_usage(per_module_ram_usage):
    print("plotting ram usage")

    timestamps = np.array(per_module_ram_usage["timestamp"])
    exp_duration_s, exp_duration_txt = timestamps_to_duration(timestamps)

    labels = (np.array(timestamps) - np.min(timestamps)) * 1e-9

    def _plot_line_ram_usage(excepted):
        ax = plt.gca()
        for idx, (process, mem_use) in enumerate(
                {k: per_module_ram_usage[k] for k in
                 set(per_module_ram_usage.keys()) - excepted}.items()):  # exclude bgpd (sum of the others) timestamps (not data values)
            ax.plot(labels, np.array(list(map(_get_total_from_dict, mem_use))) * 1e-6, label=f"{process} daemon")
        ax.set_ylabel("Memory Usage (MB)")
        ax.set_xlabel("Time (s)")
        ax.legend()
        plt.title(make_plot_title(f"FRR Modules Memory Usage Monitoring\n w/o {' '.join(list(map(str, excepted)))}",
                                  len(labels), exp_duration_txt, "Timeseries"))
        plt.show()

    # plot all excepted timestamps which aren't y values
    _plot_line_ram_usage({"timestamp"})
    # plot all excepted libfrr which hosts the route tables
    _plot_line_ram_usage({"timestamp", "bgpd"})

    bmp_usage = np.array(
        list(
            map(
                lambda x: x.get("total"),
                per_module_ram_usage["bmp"]
            )
        )
    )

    bgpd_usage = np.array(
        list(
            map(
                lambda x: x.get("total"),
                per_module_ram_usage["bgpd"]
            )
        )
    )

    # per_vrf_logmem_usage
    # [
    #     [ [timestamp1, mem_usage1], [timestamp2, mem_usage2], ... ] for vrf i
    #     [...] for vrf i + 1
    # ]

    logmem_usage_sum = benchmark_logs_get_ram_usage(per_module_ram_usage)

    nbin = 100
    bins, bmp_values = make_bins_from_timeseries(bmp_usage, nbin=nbin)
    bins, bgp_values = make_bins_from_timeseries(bgpd_usage, nbin=nbin)
    _, logmem_values = make_bins_from_timeseries(logmem_usage_sum, nbin=nbin)
    bins = scale_x_axis_for_duration(bins, exp_duration_s)

    plt.bar(bins, bmp_values * 1e-6, label="bmp module ram usage", color=plt_colors[1])
    plt.legend()
    plt.ylabel("Memory Usage (MB)")
    plt.xlabel("Time (s)")
    plt.title(make_plot_title("FRR BMP RAM Usage Monitoring", len(labels), exp_duration_txt, "Timeseries"))
    plt.show()

    ax = plt.gca()
    unit_scale = 1e-6
    bgpd_values_scaled = bgp_values * unit_scale
    bmp_values_scaled = bmp_values * unit_scale
    logmem_values = logmem_values * unit_scale
    ax.bar(bins,
           logmem_values,
           label="lmlogs library"
           )
    ax.bar(bins,
           bgpd_values_scaled,
           bottom=logmem_values,
           label="bgpd process"
           )
    ax.bar(bins,
           bmp_values_scaled,
           bottom=bgpd_values_scaled + logmem_values,
           label="bmp module"
           )
    ax.set_ylabel("Memory Usage (MB)")
    ax.set_xlabel("Time (s)")
    ax.legend()
    plt.title(make_plot_title("FRR BGP+BMP+lmlogs Cumulative RAM Usage", len(labels), exp_duration_txt, "Timeseries"))
    plt.show()

    ax = plt.gca()
    ax.bar(bins,
           bgpd_values_scaled,
           label="bgpd"
           )
    ax.bar(bins,
           bmp_values_scaled,
           bottom=bgpd_values_scaled,
           label="bmp module"
           )
    ax.set_ylabel("Memory Usage (MB)")
    ax.set_xlabel("Time (s)")
    ax.legend()
    plt.title(make_plot_title("FRR BGP+BMP Cumulative RAM Usage", len(labels), exp_duration_txt, "Timeseries"))
    plt.show()

    # plot logging system ram usage
    logs_ram_usage_values = benchmark_logs_get_ram_usage(per_module_ram_usage)
    fig_logmem_lineplot, axs_logmem_lineplot = plt.gcf(), plt.gca()
    axs_logmem_lineplot.plot(make_x_axis_for_timeseries(logs_ram_usage_values, exp_duration_s),
                                logs_ram_usage_values,
                                color=plt_colors[0],
                                label='logs ram usage (bytes)')
    plot_data_stats(axs_logmem_lineplot, logs_ram_usage_values, data_label="logs ram usage (bytes)", mean=True,
                    median=True)

    axs_logmem_lineplot.set_title(
        make_plot_title(f"FRR Benchmark Logging System Memory Usage", len(logs_ram_usage_values),
                        exp_duration_txt, "Timeseries"))
    axs_logmem_lineplot.set_xlabel("Time (s)")
    axs_logmem_lineplot.set_ylabel("Memory usage (bytes)")
    axs_logmem_lineplot.legend()
    fig_logmem_lineplot.show()

    # bmp usage repartition by memory types
    def _get_serie_of_column(module_memuse, column):
        titles = functools.reduce(lambda s1, s2: s1.union(s2),
                                  [set(_get_column_titles(memuse.get("details")[1:] or [])) for memuse in
                                   module_memuse])
        default_vals = {k: 0 for k in titles}

        result = [default_vals | dict(zip(_get_column_titles(detail)[1:],
                                          list(map(int, _get_column(detail, _get_col_index(column))[1:])))) if len(
            detail) > 0 else default_vals for detail in
                  [[] if x == [] else x for x in map(lambda x: x.get("details"), module_memuse)]]

        return titles, result, default_vals

    for column_type in set(_get_column_types().keys()) - {"size"}:
        _, result, _ = _get_serie_of_column(per_module_ram_usage["bmp"], column_type)
        for name, values in json_obj_split_series(result).items():
            plt.plot(make_x_axis_for_timeseries(values, exp_duration_s), values, label=name)

        plt.legend()
        plt.title(
            make_plot_title("BMP Memory Use Repartition across datatypes", len(result), exp_duration_txt, "Timeseries"))
        plt.ylabel(_get_column_display_name(column_type))
        plt.xlabel("Time (s)")
        plt.show()


def plot_cpu_usage(per_module_ram_usage, logs, matches):
    print("plotting cpu usage")
    fig_cpu_lineplot, axs_cpu_lineplot = plt.subplots(len(logs), 1, figsize=(6.4, 2.5 * 4.8))
    fig_hist, axs_hist = plt.subplots(len(logs), 1, figsize=(6.4, 2.5 * 4.8))
    for i, vrf_log in enumerate(logs):
        match = matches[i]

        # for each in/out msg match -> (in.timestamp, time_between(in, out))
        in_timestamp__duration = lambda: map(lambda t: (
            t[0].get("timestamp") * 1e-9, (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9),
                                             filter(lambda t: None not in t, match))
        # split timestamps and cpu_delay. keep on cpu_delay
        cpu_delay = list(zip(*in_timestamp__duration()))[1]

        total_sample_count = len(cpu_delay)
        total_time_spent = sum(list(in_timestamp__duration())[-1]) - match[0][0].get("timestamp") * 1e-9
        total_time_spent_txt = format_duration_for_display(total_time_spent)

        axs_cpu_lineplot[i].plot(make_x_axis_for_timeseries(cpu_delay, total_time_spent), cpu_delay,
                                 color=plt_colors[0], label='cpu latency (ms)')
        plot_data_stats(axs_cpu_lineplot[i], cpu_delay, data_label="cpu latency (ms)", mean=True,
                        median=True)

        # titles & styling
        title = f"VRF {i}\nFRR Daemons Update/Withdraw Processing Delay"
        axs_cpu_lineplot[i].set_title(make_plot_title(title, total_sample_count, total_time_spent_txt, "Timeseries"))
        axs_cpu_lineplot[i].set_xlabel("Time (s)")
        axs_cpu_lineplot[i].set_ylabel("Duration (ms)")
        axs_cpu_lineplot[i].legend()

        # cpu delay histogram
        axs_hist[i].hist(np.array(cpu_delay) * 1e3, bins=50, label="cpu delay (ms)")
        axs_hist[i].set_title(make_plot_title(title, total_sample_count, total_time_spent_txt, "Histogram"))
        axs_hist[i].set_ylabel("Count")
        axs_hist[i].set_xlabel("Duration (ms)")
        axs_hist[i].legend()

    plt.show()


plt_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
plt.rcParams['figure.constrained_layout.use'] = True


def type_to_char(entry_type):
    return {
        0: "U",
        1: "W",
        2: "N"
    }.get(entry_type)


def plot_prefixes_gantt(logs, matches, vrf_info, fig_scale=(2., 2.), tick_scale=10.):
    print("plotting gantt")
    fig, axs = plt.subplots(len(logs), 1, figsize=(6.4 * fig_scale[0], fig_scale[1] * 4.8))
    for i, vrf_log in enumerate(logs):
        match = matches[i]

        def in_timestamp__duration__out_timestamp():
            return map(lambda t: (
                t[0].get("prefix"),
                "{}->{}".format(type_to_char(t[0].get("type")), type_to_char(t[1].get("type"))),
                t[0].get("timestamp") * 1e-9,
                (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9,
                t[1].get("timestamp") * 1e-9),
                       filter(lambda t: None not in t, match))

        prefix, withdraws, in_timestamp, duration, out_timestamp = zip(*in_timestamp__duration__out_timestamp())

        index_to_prefix = list(set(map(lambda t: t[0].get("prefix"), filter(lambda t: None not in t, match))))
        prefix_to_index = {prefix: index for index, prefix in enumerate(index_to_prefix)}
        counters = {prefix: 0 for prefix in index_to_prefix}

        def _get_height_for_prefix(p):
            x = prefix_to_index[p] + (counters[p] / tick_scale) % 1.0
            counters[p] += 1
            return x

        series_settings = {"W->W": "red", "U->U": "green", "U->W": "orange", "W->U": "blue"}

        def _color_mapper(x):
            return series_settings.get(x)

        color_mapper = np.vectorize(_color_mapper)

        # TODO map height to ingress peer_id

        legend_elements = [Line2D([], [], label=lbl, color=clr) for lbl, clr in series_settings.items()]
        plt.legend(handles=legend_elements, )

        y = list(map(_get_height_for_prefix, prefix))
        axs[i].hlines(y, in_timestamp, out_timestamp, colors=color_mapper(withdraws), lw=2.5)
        axs[i].hlines([index - 0.05 for index in range(len(prefix_to_index))], 0, 1e40, colors="grey", lw=0.5)

        def _labels_formatter(y, pos):
            return index_to_prefix.__getitem__(int(y)) if 0 <= int(y) < len(index_to_prefix) and int(y) == y else ""

        axs[i].set_ylim([0, len(prefix_to_index)])
        axs[i].set_xlim([0, np.max(out_timestamp)])
        axs[i].yaxis.set_major_formatter(
            ticker.FuncFormatter(_labels_formatter))
    plt.show()


def load_vrf_info(logs):
    return {log["router_id"]: {k: log[k] for k in ["router_id", "vrf", "vrf_name"]} for log in logs}


def main():
    logs, ram_usage = load_files_from_router("uut")

    if len(ram_usage) > 0:
        per_module_ram_usage = json_obj_split_series(ram_usage)
        per_module_ram_usage = per_module_ram_usage | json_obj_split_series(per_module_ram_usage.get("usage"))
        per_module_ram_usage.pop("usage")
    else:
        dummy = {"total": -69, "details": []}
        per_module_ram_usage = {"timestamp": [0], "bmp": [dummy], "bgpd": [dummy], "lmlogs": [dummy]}

    for log in logs:
        pass  # log["events"] = list(filter(lambda x: not bool(x.get("leak")), log["events"]))

    vrf_info = load_vrf_info(logs)
    print("vrf infos: \n", vrf_info)

    plot_ram_usage(per_module_ram_usage)

    print("matching events")
    matches = [match_events(vrf_info, vrf_log) for vrf_log in logs]
    for log in matches:
        for match in log:
            if None in match: continue
            if match[0].get("type") not in [0, 1] and match[1].get("type") not in [0, 1]:
                print("CAREFUL! NOT ONLY UPDATE AND WITHDRAW")
            if 0 in [match[0].get("successful"), match[1].get("successful")]:
                print("CAREFUL! NOT ONLY SUCCESSFUL MESSAGES")

    plot_cpu_usage(per_module_ram_usage, logs, matches)

    plot_prefixes_gantt(logs, matches, vrf_info, fig_scale=(3., 10.))

    for match in matches:
        def in_timestamp__duration__out_timestamp():
            return map(lambda t: (
                t[0].get("prefix"),
                "{}->{}".format(type_to_char(t[0].get("type")), type_to_char(t[1].get("type"))),
                t[0].get("timestamp") * 1e-9,
                (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9,
                t[1].get("timestamp") * 1e-9,
                bool(t[0].get("leak")),
            ),
                       filter(lambda t: None not in t, match))

        prefix, withdraws, in_timestamp, duration, out_timestamp, leak = zip(*in_timestamp__duration__out_timestamp())
        data = np.array(list(zip(duration, leak)))
        # [np.mean(data[data[:, 1] == x], axis=0) for x in [0, 1]]
        # => leaking is faster than sending messages => makes sense


if __name__ == "__main__":
    main()
