#!/bin/python3
import functools
import glob
import json
import os
import re
import sys

import matplotlib.pyplot as plt
import numpy as np
from matplotlib import ticker
from matplotlib.lines import Line2D

from tests.topotests.bmp_benchmark_test.frr_memuse_log_parse import get_bmp_details_col_data, _get_column_titles, \
    _get_col_index, _get_column, _get_column_types

###

# need to ensure change when doing prefix announcements because
# ingress[UPDT/WITHDRW] => outgress[UPDT/WITHDRW] or None if no change
#

# when leaking between vrf
# which messages are from leaking ? peerid in list[router_id]
# outgress from vrf src => ingress from vrf dest => outgress to peers
# event match in vrf dest is ingress from vrf dest to outgress to peers
# need to match ingress.is_leak && !outgress.is_leak && ingress.pfx == outgress.pfx && ingress.time < outgress.time
###


CWD = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(CWD, "../"))


def load_files_from_router(rname):
    fdir = os.path.join(CWD, rname, "benchmark")
    print("Loading files from router {} in directory {}".format(rname, fdir))
    benchmark_logs = list()
    for file in glob.glob(os.path.join(fdir, "benchmark_*_vrf_*")):
        with open(file) as file:
            res = json.load(file)
            # eliminate last empty event
            res["events"] = [x for x in res.get("events") if bool(x)]
            leaks = [x for x in res.get("events") if bool(x.get("leak"))]
            # res["events"] = [x for x in res.get("events") if not bool(x.get("leak"))]
            # print(leaks)

            benchmark_logs.append(res)

    ram_usage = []
    for file in glob.glob(os.path.join(fdir, "ram_usage")):
        with open(file) as file:
            ram_usage = json.load(file)

    return benchmark_logs, ram_usage


# take object {timestamp:number, events:[]}
# "normalize" timestamp (set time reference to lowest event timestamp)
# return 2 list of all ingress and outgress events separated
def unzip_ingress_outgress(vrf_log_json):
    def _normalize_timestamp(event, ref_timestamp):
        event["timestamp"] = event["timestamp"] - ref_timestamp
        return event

    ref_timestamp = vrf_log_json.get("events")[0].get("timestamp")
    return [_normalize_timestamp(x, ref_timestamp) for x in vrf_log_json.get("events") if x.get("ingress") == 1], \
           [_normalize_timestamp(x, ref_timestamp) for x in vrf_log_json.get("events") if
            x.get("ingress") == 0 and x.get("leak") == 0], \
           [x for x in vrf_log_json.get("events")]


# take object {timestamp:number, events:[])}
# separate ingress and outgress
# find next outgress msg related to prefix of each ingress msg
# return [ingress, next_outgress]
def match_events(vrf_info, vrf_log_json):
    def _find_prefix_after_msg(events_list, trig_event):
        return next(filter(
            lambda event: event.get("timestamp") >= trig_event.get("timestamp")
                          and event.get("prefix") == trig_event.get("prefix")
                          # after a non leak update/withdraw we expect
                          #     either a leak to another vrf
                          #     or an outgoing update/withdraw for the same afi:safi
                          and (_is_leak(event) or
                               (not _is_leak(event) and event.get("afi_safi") == trig_event.get("afi_safi"))
                               ),
            events_list), None)

    def _is_leak(event):
        return bool(event.get("leak"))

    def _find_prefix_after_leak(events_all, trig_event):
        # ingress.is_leak && !outgress.is_leak && ingress.pfx == outgress.pfx && ingress.time < outgress.time
        return next(filter(
            lambda event: _is_leak(trig_event)
                          and not _is_leak(event)
                          and event.get("timestamp") >= trig_event.get("timestamp")
                          and event.get("prefix") == trig_event.get("prefix")
                          and event.get("afi_safi") == trig_event.get("afi_safi"),
            # TODO add rd equals check (shouldn't change anything)
            events_all), None)

    ingress, outgress, normalized_all = unzip_ingress_outgress(vrf_log_json)

    return [(ingress_x, (_find_prefix_after_msg(outgress, ingress_x)
                         if not _is_leak(ingress_x)
                         else _find_prefix_after_leak(normalized_all, ingress_x)
                         )) for ingress_x in ingress]


# take {timestamp:number, events:[]}
# get every mem_usage field in each event
# return list of mem_usage values (memory usage of logging system)
def benchmark_logs_get_ram_usage(vrf_log_json):
    return list(map(lambda ev: ev.get("mem_usage"), vrf_log_json.get("events")))


# take list [{},{},{}] with n fields in each list member
# return dict with n lists containing all values of the same key, in order mapped to list member keys
def json_obj_split_series(obj):
    return dict(zip(obj[0].keys(), zip(*[r.values() for r in obj])))


def make_bins_from_timeseries(values, nbin=None, binsize=None):
    if nbin is None and binsize is None:
        raise ValueError()

    values = np.array(values)
    bin_size = values.size / nbin
    indexes = np.arange(0, nbin * int(np.ceil(bin_size))).reshape((nbin, int(np.ceil(bin_size))))
    binned = np.where(indexes < values.size, np.take(values, indexes, mode="clip"), -1)
    undefined = np.any(binned < 0, axis=1)
    binned[undefined] = np.where(binned[undefined],
                                 np.nan_to_num(np.mean(binned[undefined], where=binned[undefined] >= 0), nan=0),
                                 binned[undefined])

    return np.array(list(range(nbin))), np.mean(binned, axis=1)


def plot_ram_usage(ram_usage):
    print("plotting ram usage")
    per_module_ram_usage = json_obj_split_series(ram_usage)
    per_module_ram_usage = per_module_ram_usage | json_obj_split_series(per_module_ram_usage.get("usage"))
    per_module_ram_usage.pop("usage")

    ax = plt.gca()
    labels = (np.array(per_module_ram_usage["timestamp"]) - np.min(per_module_ram_usage["timestamp"])) * 1e-6
    for idx, (process, mem_use) in enumerate(
            {k: per_module_ram_usage[k] for k in
             set(per_module_ram_usage.keys()) - {"timestamp", "staticd", "zebra"}}.items()):
        ax.plot(labels, np.array(list(map(lambda x: x.get("total"), mem_use))) * 1e-6, label=f"{process} daemon")
    ax.set_ylabel("Memory Usage (MB)")
    ax.legend()
    plt.suptitle("FRR Modules Memory Usage Monitoring")
    plt.title("{} samples over {} seconds".format(len(labels), np.max(
        per_module_ram_usage["timestamp"] - np.min(per_module_ram_usage["timestamp"])) * 1e-9))
    plt.show()

    bmp_usage = np.array(
        list(
            map(
                lambda x: x.get("total"),
                per_module_ram_usage["bmp"]
            )
        )
    )

    other_bgpd_usage = np.array(
        list(
            map(
                lambda x: x.get("total"),
                per_module_ram_usage["bgpd"]
            )
        )
    )

    bins, bmp_values = make_bins_from_timeseries(bmp_usage, nbin=100)
    bins, bgpd_values = make_bins_from_timeseries(other_bgpd_usage, nbin=100)

    plt.bar(bins, bmp_values, label="bmp module ram usage", color=plt_colors[1])
    plt.legend()
    plt.suptitle("FRR BMP RAM Usage Monitoring")
    plt.title("{} samples over {} seconds".format(len(labels), np.max(
        per_module_ram_usage["timestamp"] - np.min(per_module_ram_usage["timestamp"])) * 1e-9))
    plt.show()

    ax = plt.gca()
    ax.bar(bins,
           bgpd_values,
           label="bgpd ram usage"
           )
    ax.bar(bins,
           bmp_values,
           bottom=bgpd_values,
           label="bmp module ram usage"
           )
    ax.set_ylabel("BMP Memory Usage (MB)")
    ax.legend()
    plt.suptitle("FRR BMP RAM Usage Monitoring")
    plt.title("{} samples over {} seconds".format(len(labels), np.max(
        per_module_ram_usage["timestamp"] - np.min(per_module_ram_usage["timestamp"])) * 1e-9))
    plt.show()

    # bmp usage repartition by memory types
    def _get_serie_of_column(module_memuse, column):
        titles = functools.reduce(lambda s1, s2: s1.union(s2),
                                  [set(_get_column_titles(memuse.get("details")[1:] or [])) for memuse in
                                   module_memuse])
        default_vals = {k: 0 for k in titles}

        result = [default_vals | dict(zip(_get_column_titles(detail)[1:],
                                          list(map(int, _get_column(detail, _get_col_index(column))[1:])))) if len(
            detail) > 0 else default_vals for detail in
                  [[] if x == [] else x for x in map(lambda x: x.get("details"), module_memuse)]]

        return titles, result, default_vals

    for column_type in set(_get_column_types().keys()) - {"size"}:
        _, result, _ = _get_serie_of_column(per_module_ram_usage["bmp"], column_type)
        for name, values in json_obj_split_series(result).items():
            plt.plot(values, label=name)

        plt.legend()
        plt.title(f'BMP Memory Use Repartition across datatypes, criterion="{column_type}"')
        plt.show()


def plot_cpu_usage(logs, matches):
    print("plotting cpu usage")
    fig, axs = plt.subplots(len(logs), 1, figsize=(6.4, 2. * 4.8))
    for i, vrf_log in enumerate(logs):
        match = matches[i]

        # for each in/out msg match -> (in.timestamp, time_between(in, out))
        in_timestamp__duration = lambda: map(lambda t: (
            t[0].get("timestamp") * 1e-9, (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9),
                                             filter(lambda t: None not in t, match))
        # split timestamps and durations. keep on durations
        durations = list(zip(*in_timestamp__duration()))[1]
        ln1 = axs[i].plot(durations, color=plt_colors[0], label='cpu latency (ms)')
        axs[i].tick_params(axis='y', labelcolor=plt_colors[0])

        ax2 = axs[i].twinx()
        ln2 = ax2.plot(benchmark_logs_get_ram_usage(vrf_log)[:len(durations)], color=plt_colors[1],
                       label='logs ram usage (bytes)')
        ax2.tick_params(axis='y', labelcolor=plt_colors[1])

        lns = ln1 + ln2
        fig.legend(lns, [ln.get_label() for ln in lns])

        axs[i].set_title("FRR Daemons CPU Usage Monitoring\n{} samples over {} seconds".format(len(durations),
                                                                                               sum(list(
                                                                                                   in_timestamp__duration())[
                                                                                                       -1]) -
                                                                                               match[0][0].get(
                                                                                                   "timestamp") * 1e-9))

    plt.show()


plt_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']


def type_to_char(entry_type):
    return {
        0: "U",
        1: "W",
        2: "N"
    }.get(entry_type)


def plot_prefixes_gantt(logs, matches, vrf_info, fig_scale=(2., 2.), tick_scale=10.):
    print("plotting gantt")
    fig, axs = plt.subplots(len(logs), 1, figsize=(6.4 * fig_scale[0], fig_scale[1] * 4.8))
    for i, vrf_log in enumerate(logs):
        match = matches[i]

        def in_timestamp__duration__out_timestamp():
            return map(lambda t: (
                t[0].get("prefix"),
                "{}->{}".format(type_to_char(t[0].get("type")), type_to_char(t[1].get("type"))),
                t[0].get("timestamp") * 1e-9,
                (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9,
                t[1].get("timestamp") * 1e-9),
                       filter(lambda t: None not in t, match))

        prefix, withdraws, in_timestamp, duration, out_timestamp = zip(*in_timestamp__duration__out_timestamp())

        index_to_prefix = list(set(map(lambda t: t[0].get("prefix"), filter(lambda t: None not in t, match))))
        prefix_to_index = {prefix: index for index, prefix in enumerate(index_to_prefix)}
        counters = {prefix: 0 for prefix in index_to_prefix}

        def _get_height_for_prefix(p):
            x = prefix_to_index[p] + (counters[p] / tick_scale) % 1.0
            counters[p] += 1
            return x

        series_settings = {"W->W": "red", "U->U": "green", "U->W": "orange", "W->U": "blue"}

        def _color_mapper(x):
            return series_settings.get(x)

        color_mapper = np.vectorize(_color_mapper)

        # TODO map height to ingress peer_id

        legend_elements = [Line2D([], [], label=lbl, color=clr) for lbl, clr in series_settings.items()]
        plt.legend(handles=legend_elements)

        y = list(map(_get_height_for_prefix, prefix))
        axs[i].hlines(y, in_timestamp, out_timestamp, colors=color_mapper(withdraws), lw=2.5)
        axs[i].hlines([index - 0.05 for index in range(len(prefix_to_index))], 0, 1e40, colors="grey", lw=0.5)

        def _labels_formatter(y, pos):
            return index_to_prefix.__getitem__(int(y)) if 0 <= int(y) < len(index_to_prefix) and int(y) == y else ""

        axs[i].set_ylim([0, len(prefix_to_index)])
        axs[i].set_xlim([0, np.max(out_timestamp)])
        axs[i].yaxis.set_major_formatter(
            ticker.FuncFormatter(_labels_formatter))
    plt.gcf().tight_layout()
    plt.show()


def load_vrf_info(logs):
    return {log["router_id"]: {k: log[k] for k in ["router_id", "vrf", "vrf_name"]} for log in logs}


def main():
    # TODO add prefix ticks
    logs, ram_usage = load_files_from_router("uut")
    for log in logs:
        pass  # log["events"] = list(filter(lambda x: not bool(x.get("leak")), log["events"]))

    vrf_info = load_vrf_info(logs)
    print("vrf infos: \n", vrf_info)

    plot_ram_usage(ram_usage)

    print("matching events")
    matches = [match_events(vrf_info, vrf_log) for vrf_log in logs]
    for log in matches:
        for match in log:
            if None in match: continue
            if match[0].get("type") not in [0, 1] and match[1].get("type") not in [0, 1]:
                print("CAREFUL! NOT ONLY UPDATE AND WITHDRAW")
            if 0 in [match[0].get("successful"), match[1].get("successful")]:
                print("CAREFUL! NOT ONLY SUCCESSFUL MESSAGES")

    plot_prefixes_gantt(logs, matches, vrf_info, fig_scale=(3., 10.))

    plot_cpu_usage(logs, matches)

    for match in matches:
        def in_timestamp__duration__out_timestamp():
            return map(lambda t: (
                t[0].get("prefix"),
                "{}->{}".format(type_to_char(t[0].get("type")), type_to_char(t[1].get("type"))),
                t[0].get("timestamp") * 1e-9,
                (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9,
                t[1].get("timestamp") * 1e-9,
                bool(t[0].get("leak")),
            ),
                       filter(lambda t: None not in t, match))

        prefix, withdraws, in_timestamp, duration, out_timestamp, leak = zip(*in_timestamp__duration__out_timestamp())
        data = np.array(list(zip(duration, leak)))
        # [np.mean(data[data[:, 1] == x], axis=0) for x in [0, 1]]
        # => leaking is faster than sending messages => makes sense


if __name__ == "__main__":
    main()
