#!/bin/python3
import functools
import glob
import json
import operator
import os
import sys

import matplotlib.figure
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import ticker, scale
from matplotlib.lines import Line2D

from tests.topotests.bmp_benchmark_test.frr_memuse_log_parse import _get_column_types, _get_column_display_name, \
    get_modules_datatypes_and_sizes, \
    _get_series_of_column

###

# need to ensure change when doing prefix announcements because
# ingress[UPDT/WITHDRW] => outgress[UPDT/WITHDRW] or None if no change
#

# when leaking between vrf
# which messages are from leaking ? peerid in list[router_id]
# outgress from vrf src => ingress from vrf dest => outgress to peers
# event match in vrf dest is ingress from vrf dest to outgress to peers
# need to match ingress.is_leak && !outgress.is_leak && ingress.pfx == outgress.pfx && ingress.time < outgress.time
###


CWD = os.path.dirname(os.path.realpath(__file__))
sys.path.append(os.path.join(CWD, "../"))


def load_files_from_router(test_name, rname):
    fdir = os.path.join(CWD, "out", "logs", test_name, rname)
    print("Loading files from router {} in directory {}".format(rname, fdir))
    benchmark_logs = list()
    for file in glob.glob(os.path.join(fdir, "benchmark_*_vrf_*")):
        with open(file) as file:
            res = json.load(file)
            # eliminate last empty event
            res["events"] = [x for x in res.get("events") if bool(x)]
            benchmark_logs.append(res)
            print(f"loaded {file.name}")

    ram_usage = []
    for file in glob.glob(os.path.join(fdir, "ram_usage")):
        with open(file) as file:
            ram_usage = json.load(file)
            print(f"loaded {file.name}")

    return benchmark_logs, ram_usage


# take object {timestamp:number, events:[]}
# "normalize" timestamp (set time reference to lowest event timestamp)
# return 2 list of all event_start and event_end events separated
def unzip_event_start_event_end(vrf_log_json):
    def _normalize_timestamp(event, ref_timestamp):
        event["timestamp"] = event["timestamp"] - ref_timestamp
        return event

    ref_timestamp = vrf_log_json.get("events")[0].get("timestamp")

    # we want leak == 0 in event_end because :
    #   1. vpn recv ipv4 uni update for prefixA  => leak=false event_begin=true type=UPD afi_safi=ipv4Uni prefix=prefixA
    #   2. vpn leak to default                   => leak=true  event_begin=false type=UPD afi_safi=ipv4Uni prefix=prefixA
    #   3. vpn send ipv4 uni update for to peers => leak=false event_begin=false type=UPD afi_safi=ipv4Uni prefix=prefixA
    # and we don't want to consider the end of the event to be the leak instead of the msg snd to peers
    return [_normalize_timestamp(x, ref_timestamp) for x in vrf_log_json.get("events") if x.get("event_begin") == 1], \
           [_normalize_timestamp(x, ref_timestamp) for x in vrf_log_json.get("events") if
            x.get("event_begin") == 0 and x.get("leak") == 0], \
           [_normalize_timestamp(x, ref_timestamp) if x.get("timestamp") >= ref_timestamp else x for x in
            vrf_log_json.get("events")]


# take object {timestamp:number, events:[])}
# separate event_start and event_end
# find next event_end msg related to prefix of each event_start msg
# return [event_start, next_event_end]
def match_events(vrf_info, vrf_log_json):
    def _find_prefix_after_msg(events_list, trig_event):
        return next(filter(
            lambda event: event.get("timestamp") >= trig_event.get("timestamp")
                          and event.get("prefix") == trig_event.get("prefix")
                          #     after a non leak update/withdraw we expect
                          #     either a leak to another vrf
                          #     or an outgoing update/withdraw for the same afi:safi
                          and (_is_leak(event) or
                               (not _is_leak(event) and event.get("afi_safi") == trig_event.get("afi_safi"))
                               ),
            events_list), None)

    def _is_leak(event):
        return bool(event.get("leak"))

    def _find_prefix_after_leak(events_all, trig_event):
        # ingress.is_leak && !outgress.is_leak && ingress.pfx == outgress.pfx && ingress.time < outgress.time
        return next(filter(
            lambda event: _is_leak(trig_event)
                          and not _is_leak(event)
                          and event.get("timestamp") >= trig_event.get("timestamp")
                          and event.get("prefix") == trig_event.get("prefix")
                          and event.get("afi_safi") == trig_event.get("afi_safi"),
            # TODO add rd equals check (shouldn't change anything)
            events_all), None)

    if len(vrf_log_json.get("events")) == 0:
        return []

    event_start, event_end, normalized_all = unzip_event_start_event_end(vrf_log_json)

    return [(event_start_x, (_find_prefix_after_msg(event_end, event_start_x)
                             if not _is_leak(event_start_x)
                             else _find_prefix_after_leak(normalized_all, event_start_x)
                             )) for event_start_x in event_start]


# take [{"total": int, "details": []}]
# get every mem_usage field in each event
# return list of mem_usage values (memory usage of logging system)
def benchmark_logs_get_ram_usage(lmlogs_module_ram_usage):
    return list(map(_get_total_from_dict, lmlogs_module_ram_usage["lmlogs"]))


# take list [{},{},{}] with n fields in each list member
# return dict with n lists containing all values of the same key, in order mapped to list member keys
def json_obj_split_series(obj):
    return dict(zip(obj[0].keys(), zip(*[r.values() for r in obj])))


def make_bins_from_timeseries(values, nbin=None, binsize=None):
    if nbin is None and binsize is None:
        raise ValueError()

    values = np.array(values)
    bin_size = values.size / nbin
    indexes = np.arange(0, nbin * int(np.ceil(bin_size))).reshape((nbin, int(np.ceil(bin_size))))
    binned = np.where(indexes < values.size, np.take(values, indexes, mode="clip"), -1)
    undefined = np.any(binned < 0, axis=1)
    binned[undefined] = np.where(binned[undefined],
                                 np.nan_to_num(np.mean(binned[undefined], where=binned[undefined] >= 0), nan=0),
                                 binned[undefined])

    return np.array(list(range(nbin))), np.mean(binned, axis=1)


def format_number_for_display(duration):
    return str(np.round(duration, 2))


def timestamps_to_duration(timeseries_timestamps):
    duration_s = np.max(timeseries_timestamps - np.min(timeseries_timestamps)) * 1e-9
    return duration_s, format_number_for_display(duration_s)


def scale_x_axis_for_duration(x_axis, duration):
    return x_axis * duration / float(len(x_axis))


def make_x_axis_for_timeseries(y_values, duration):
    return scale_x_axis_for_duration(np.arange(len(y_values)), duration)


def make_plot_title(title, sample_count, duration_txt, tyqe):
    return f"{title}\n{sample_count} samples over {duration_txt} seconds\n{tyqe}"


def plot_data_stats(axis, data, data_label=None, with_line=True, mean=False, median=False, peak=False):
    if with_line:
        color = plt_colors[len(axis.lines) % len(plt_colors)]
    else:
        color = None

    def _plot_stat_line(axis, stat, color, data_label, label_pfx):
        lbl = f"{label_pfx} {data_label} = {format_number_for_display(stat)}" if data_label is not None else None
        if with_line:
            axis.axhline(y=stat, color=color, label=lbl)
        else:
            line2d = Line2D([], [], label=lbl)
            axis.add_line(line2d)

    if mean:
        mean = np.nanmean(data)
        _plot_stat_line(axis, mean, color, data_label, "mean of")

    if median:
        median = np.nanmedian(data)
        _plot_stat_line(axis, median, color, data_label, "median of")

    if peak:
        peak = np.max(data)
        _plot_stat_line(axis, peak, color, data_label, "peak of")


def sync_timeseries(reference, series_list):
    return np.array([np.interp(reference, series[:, 0], series[:, 1]) for series in series_list])


_get_total_from_dict = functools.partial(lambda key, dic: dic[key], "total")


def plot_ram_usage(per_module_ram_usage):
    print("plotting ram usage")

    timestamps = np.array(per_module_ram_usage["timestamp"])
    exp_duration_s, exp_duration_txt = timestamps_to_duration(timestamps)

    labels = (np.array(timestamps) - np.min(timestamps)) * 1e-9

    def _plot_line_ram_usage(excepted):
        ax = plt.gca()
        for idx, (process, mem_use) in enumerate(
                {k: per_module_ram_usage[k] for k in
                 set(per_module_ram_usage.keys()) - excepted}.items()):  # exclude bgpd (sum of the others) timestamps (not data values)
            data = np.array(list(map(_get_total_from_dict, mem_use))) * 1e-6
            lbl = f"{process} daemon"
            ax.plot(labels, data, label=lbl)
            plot_data_stats(ax, data, data_label=lbl, median=True, mean=True, peak=True, with_line=False)
        ax.set_ylabel("Memory Usage (MB)")
        ax.set_xlabel("Time (s)")
        ax.legend()
        plt.title(make_plot_title(f"FRR Modules Memory Usage Monitoring\n w/o {' '.join(list(map(str, excepted)))}",
                                  len(labels), exp_duration_txt, "Timeseries"))
        render_plot(plt.gcf())

    # plot all excepted timestamps which aren't y values
    _plot_line_ram_usage({"timestamp"})
    # plot all excepted libfrr which hosts the route tables
    _plot_line_ram_usage({"timestamp", "bgpd"})

    bmp_usage = np.array(
        list(
            map(
                lambda x: x.get("total"),
                per_module_ram_usage.get("bmp") or [{"total": 0}] * len(per_module_ram_usage['bgpd'])
            )
        )
    )

    bgpd_usage = np.array(
        list(
            map(
                lambda x: x.get("total"),
                per_module_ram_usage["bgpd"]
            )
        )
    )

    # per_vrf_logmem_usage
    # [
    #     [ [timestamp1, mem_usage1], [timestamp2, mem_usage2], ... ] for vrf i
    #     [...] for vrf i + 1
    # ]

    logmem_usage_sum = benchmark_logs_get_ram_usage(per_module_ram_usage)

    nbin = 100
    bins, bmp_values = make_bins_from_timeseries(bmp_usage, nbin=nbin)
    bins, bgp_values = make_bins_from_timeseries(bgpd_usage, nbin=nbin)
    _, logmem_values = make_bins_from_timeseries(logmem_usage_sum, nbin=nbin)
    bins = scale_x_axis_for_duration(bins, exp_duration_s)

    plt.bar(bins, bmp_values * 1e-6, label="bmp module ram usage", color=plt_colors[1])
    plt.legend()
    plt.ylabel("Memory Usage (MB)")
    plt.xlabel("Time (s)")
    plt.title(make_plot_title("FRR BMP RAM Usage Monitoring", len(labels), exp_duration_txt, "Timeseries"))
    render_plot(plt.gcf())

    ax = plt.gca()
    unit_scale = 1e-6
    bgpd_values_scaled = bgp_values * unit_scale
    bmp_values_scaled = bmp_values * unit_scale
    logmem_values = logmem_values * unit_scale
    ax.bar(bins,
           logmem_values,
           label="lmlogs library"
           )
    ax.bar(bins,
           bgpd_values_scaled,
           bottom=logmem_values,
           label="bgpd process"
           )
    ax.bar(bins,
           bmp_values_scaled,
           bottom=bgpd_values_scaled + logmem_values,
           label="bmp module"
           )
    ax.set_ylabel("Memory Usage (MB)")
    ax.set_xlabel("Time (s)")
    ax.legend()
    plt.title(make_plot_title("FRR BGP+BMP+lmlogs Cumulative RAM Usage", len(labels), exp_duration_txt, "Timeseries"))
    render_plot(plt.gcf())

    ax = plt.gca()
    ax.bar(bins,
           bgpd_values_scaled,
           label="bgpd"
           )
    ax.bar(bins,
           bmp_values_scaled,
           bottom=bgpd_values_scaled,
           label="bmp module"
           )
    ax.set_ylabel("Memory Usage (MB)")
    ax.set_xlabel("Time (s)")
    plt.title(make_plot_title("FRR BGP+BMP Cumulative RAM Usage", len(labels), exp_duration_txt, "Timeseries"))
    plot_data_stats(ax, bgpd_values_scaled, data_label="bgpd module", mean=True, peak=True, median=True)
    plot_data_stats(ax, bgpd_values_scaled + bmp_values_scaled, data_label="bmp module", mean=True, median=True, peak=True)
    ax.legend()
    render_plot(plt.gcf())

    # plot logging system ram usage
    logs_ram_usage_values = benchmark_logs_get_ram_usage(per_module_ram_usage)
    fig_logmem_lineplot, axs_logmem_lineplot = plt.gcf(), plt.gca()
    axs_logmem_lineplot.plot(make_x_axis_for_timeseries(logs_ram_usage_values, exp_duration_s),
                             logs_ram_usage_values,
                             color=plt_colors[0],
                             label='logs ram usage (bytes)')
    plot_data_stats(axs_logmem_lineplot, logs_ram_usage_values, data_label="logs ram usage (bytes)", mean=True,
                    median=True, peak=True)

    axs_logmem_lineplot.set_title(
        make_plot_title(f"FRR Benchmark Logging System Memory Usage", len(logs_ram_usage_values),
                        exp_duration_txt, "Timeseries"))
    axs_logmem_lineplot.set_xlabel("Time (s)")
    axs_logmem_lineplot.set_ylabel("Memory usage (bytes)")
    axs_logmem_lineplot.legend()
    render_plot(fig_logmem_lineplot)

    # bmp usage repartition by memory types

    for column_type in set(_get_column_types().keys()) - {"size", 'title'}:
        result = []
        if "bmp" in per_module_ram_usage:
            _, result, _ = _get_series_of_column(per_module_ram_usage["bmp"], column_type)
            for name, values in json_obj_split_series(result).items():
                plt.plot(make_x_axis_for_timeseries(values, exp_duration_s), values, label=name)

        plt.legend()
        plt.title(
            make_plot_title("BMP Memory Use Repartition across datatypes", len(result), exp_duration_txt, "Timeseries"))
        plt.ylabel(_get_column_display_name(column_type))
        plt.xlabel("Time (s)")
        render_plot(plt.gcf())

    modules_types_sizes = get_modules_datatypes_and_sizes(
        {k: v for k, v in per_module_ram_usage.items() if k not in ["timestamp"]})
    modules_types_sizes = {module: set(functools.reduce(operator.iconcat, values, [])) for module, values in
                           modules_types_sizes.items()}

    subplots_count = len(modules_types_sizes.keys())
    fig, axs = plt.subplots(1, subplots_count, figsize=(6.4 * subplots_count, 4.8 * 1.5))
    for i, (ax, (module, data)) in enumerate(zip(axs, modules_types_sizes.items())):
        data = [(name, int(size_str) if size_str.isnumeric() else 0) for (name, size_str) in data]
        data = list(zip(*sorted(data, reverse=True, key=lambda tup: tup[1])))
        if len(data) == 0:
            print(f"no data for {module} data types sizes")
            continue
        ax.barh(data[0], data[1], label="datatypes size")
        ax.set_xscale(scale.LogScale(ax, base=2))
        ax.set_title(f"{module} datatypes sizes")
        ax.set_ylabel("Size (bytes)")
        ax.set_xlabel("Datatype")
        ax.legend()
    render_plot(plt.gcf())


def plot_cpu_usage(logs, matches):
    print("plotting cpu usage")
    fig_cpu_lineplot, axs_cpu_lineplot = plt.subplots(len(logs), 1, figsize=(6.4, 2.5 * 4.8))
    fig_hist, axs_hist = plt.subplots(len(logs), 1, figsize=(6.4, 2.5 * 4.8))
    for i, vrf_log in enumerate(logs):
        match = matches[i]

        # for each in/out msg match -> (in.timestamp, time_between(in, out))
        in_timestamp__duration = lambda: map(lambda t: (
            t[0].get("timestamp") * 1e-9, (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9),
                                             filter(lambda t: None not in t, match))
        # split timestamps and cpu_delay. keep on cpu_delay
        cpu_delay = list(zip(*in_timestamp__duration()))
        if len(cpu_delay) == 0:
            return print("[cpu] no event to process")

        cpu_delay = cpu_delay[1]

        total_sample_count = len(cpu_delay)
        total_time_spent = sum(list(in_timestamp__duration())[-1]) - match[0][0].get("timestamp") * 1e-9
        total_time_spent_txt = format_number_for_display(total_time_spent)

        axs_cpu_lineplot[i].plot(make_x_axis_for_timeseries(cpu_delay, total_time_spent), cpu_delay,
                                 color=plt_colors[0], label='cpu latency (ms)')
        plot_data_stats(axs_cpu_lineplot[i], cpu_delay, data_label="cpu latency (ms)", mean=True,
                        median=True, peak=True)

        # titles & styling
        title = f"VRF {vrf_log['vrf']} {vrf_log['vrf_name']}\nFRR Daemons Update/Withdraw Processing Delay"
        axs_cpu_lineplot[i].set_title(make_plot_title(title, total_sample_count, total_time_spent_txt, "Timeseries"))
        axs_cpu_lineplot[i].set_xlabel("Time (s)")
        axs_cpu_lineplot[i].set_ylabel("Duration (ms)")
        axs_cpu_lineplot[i].legend()

        # cpu delay histogram
        axs_hist[i].hist(np.array(cpu_delay) * 1e3, bins=50, label="cpu delay (ms)")
        axs_hist[i].set_title(make_plot_title(title, total_sample_count, total_time_spent_txt, "Histogram"))
        axs_hist[i].set_ylabel("Count")
        axs_hist[i].set_xlabel("Duration (ms)")
        axs_hist[i].legend()

    render_plot(plt.gcf())


plt_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']
plt.rcParams['figure.constrained_layout.use'] = True


def type_to_char(entry_type):
    return {
        0: "U",
        1: "W",
        2: "N"
    }.get(entry_type)


def plot_prefixes_gantt(logs, matches, vrf_info, fig_scale=(2., 2.), tick_scale=10.):
    print("plotting gantt")
    fig, axs = plt.subplots(len(logs), 1, figsize=(6.4 * fig_scale[0], fig_scale[1] * 4.8))
    for i, vrf_log in enumerate(logs):
        match = matches[i]
        if not match: continue

        def in_timestamp__duration__out_timestamp():
            return map(lambda t: (
                t[0].get("prefix"),
                "{}->{}".format(type_to_char(t[0].get("type")), type_to_char(t[1].get("type"))),
                t[0].get("timestamp") * 1e-9,
                (t[1].get("timestamp") - t[0].get("timestamp")) * 1e-9,
                t[1].get("timestamp") * 1e-9),
                       filter(lambda t: None not in t, match))

        prefix, pair_type, in_timestamp, duration, out_timestamp = zip(*in_timestamp__duration__out_timestamp())

        index_to_prefix = list(set(map(lambda t: t[0].get("prefix"), filter(lambda t: None not in t, match))))
        prefix_to_index = {prefix: index for index, prefix in enumerate(index_to_prefix)}
        counters = {prefix: 0 for prefix in index_to_prefix}

        def _get_height_for_prefix(p):
            x = prefix_to_index[p] + (counters[p] / tick_scale) % 1.0
            counters[p] += 1
            return x

        series_settings = {
            "W->W": "red",
            "U->U": "green",
            "U->W": "orange",
            "W->U": "blue",
            "U->N": "purple",
            "W->N": "black"
        }

        def _color_mapper(x):
            return series_settings.get(x)

        color_mapper = np.vectorize(_color_mapper)

        # TODO map height to ingress peer_id

        legend_elements = [Line2D([], [], label=lbl, color=clr) for lbl, clr in series_settings.items()]
        plt.legend(handles=legend_elements, )

        y = list(map(_get_height_for_prefix, prefix))
        axs[i].hlines(y, in_timestamp, out_timestamp, colors=color_mapper(pair_type), lw=2.5)
        axs[i].hlines([index - 0.05 for index in range(len(prefix_to_index))], 0, 1e40, colors="grey", lw=0.5)

        def _labels_formatter(y, pos):
            return index_to_prefix.__getitem__(int(y)) if 0 <= int(y) < len(index_to_prefix) and int(y) == y else ""

        axs[i].set_ylim([0, len(prefix_to_index)])
        axs[i].set_xlim([0, np.max(out_timestamp)])

        start, end = axs[i].get_ylim()
        axs[i].yaxis.set_ticks(np.arange(start, end, 1))
        axs[i].yaxis.set_major_formatter(ticker.FuncFormatter(_labels_formatter))
        axs[i].yaxis.set_minor_formatter(ticker.FuncFormatter(_labels_formatter))

        duration_s, duration_str = timestamps_to_duration(in_timestamp + out_timestamp)
        axs[i].set_title(
            make_plot_title(f"VRF {vrf_log['vrf']} {vrf_log['vrf_name']}\nBenchmark Prefixes Update/Withdraw Timeline",
                            len(in_timestamp), duration_str, "Gantt Plot"))

    render_plot(plt.gcf())


def load_vrf_info(logs):
    return {log["router_id"]: {k: log[k] for k in ["router_id", "vrf", "vrf_name"]} for log in logs}


plot_index = 0
test_name = None


def skip_plot():
    global plot_index
    plot_index += 1


def render_plot(fig: matplotlib.figure.Figure):
    global plot_index
    global test_name
    if plt.get_fignums() == 0:
        print("no figure to plot")
        return

    out_path = os.path.join(CWD, "out", "plots", test_name, f"plot_{plot_index}.svg")
    dir, file = os.path.split(out_path)
    if dir and not os.path.exists(dir):
        os.makedirs(dir, exist_ok=True)

    fig.savefig(out_path, format="svg")
    plt.close(fig)
    plot_index += 1


figscaling = 3


def main():
    default_figsize = matplotlib.rcParamsDefault['figure.figsize']
    matplotlib.rcParams['figure.figsize'] = (default_figsize[0] * figscaling, default_figsize[1] * figscaling)

    if len(sys.argv) == 1:
        print("please specify name of the test to load logs from")
        sys.exit(1)

    test_name_arg = sys.argv[1]
    test_names_arg_opts = os.listdir(os.path.join(CWD, "out", "logs"))

    if test_name_arg not in ["*", *test_names_arg_opts]:
        opts = "\n".join(map(lambda x: f"- {x}", test_names_arg_opts))
        print(f"please specify a valid test_name from : \n{opts}")
        sys.exit(1)

    def _run_one():
        print(f"TEST {test_name}")

        global plot_index
        plot_index = 0

        logs, ram_usage = load_files_from_router(test_name, "uut")

        if len(ram_usage) > 0:
            per_module_ram_usage = json_obj_split_series(ram_usage)
            per_module_ram_usage = per_module_ram_usage | json_obj_split_series(per_module_ram_usage.get("usage"))
            per_module_ram_usage.pop("usage")
        else:
            dummy = {"total": -69, "details": []}
            per_module_ram_usage = {"timestamp": [0], "bmp": [dummy], "bgpd": [dummy], "lmlogs": [dummy]}

        vrf_info = load_vrf_info(logs)
        print("vrf infos: \n", vrf_info)

        plot_ram_usage(per_module_ram_usage)

        print("matching events")
        matches = [match_events(vrf_info, vrf_log) for vrf_log in logs]
        plot_cpu_usage(logs, matches)
        plot_prefixes_gantt(logs, matches, vrf_info, fig_scale=(3., 30.))

    global test_name
    if test_name_arg != "*":
        test_name = test_name_arg
        _run_one()
    else:
        for test_name_i in test_names_arg_opts:
            test_name = test_name_i
            _run_one()


if __name__ == "__main__":
    main()
